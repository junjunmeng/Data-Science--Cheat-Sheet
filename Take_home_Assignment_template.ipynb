{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Take home Assignment_template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junjunmeng/Data-Science--Cheat-Sheet/blob/master/Take_home_Assignment_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvgkgLnDPtqT",
        "colab_type": "text"
      },
      "source": [
        "Table of Contents\n",
        "\n",
        "*  Initial Data Analysis\n",
        "*  Data Wrangling\n",
        "*  Exploratory Data Analysis\n",
        "*  Statistical Analysis\n",
        "*  Machine Learning\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miW34qATIUKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install etsy_py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlMPDYHYHvCI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "7ae6a400-88b0-4f63-bbda-0231172a092f"
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas_profiling as pp\n",
        "# import etsy_py\n",
        "from scipy.stats import shapiro\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import kurtosis\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "import sklearn as sklearn\n",
        "from sklearn.utils import resample\n",
        "from sklearn import preprocessing as preprocessing\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report, accuracy_score, mean_squared_error\n",
        "from sklearn.metrics import precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import statsmodels.api as sm\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7tcmvIdKjdp",
        "colab_type": "text"
      },
      "source": [
        "# <font color = 'blue'> 1. Data Clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2snzLq2HvyM",
        "colab_type": "text"
      },
      "source": [
        "#### Initial Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emy6H_3oj4Bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initial_analysis(df):\n",
        "    \"\"\"\n",
        "    Given a dataframe produces a simple report on initial data analytics\n",
        "    Params:\n",
        "        - df \n",
        "    Returns:\n",
        "        - Shape of dataframe records and columns\n",
        "        - Columns and data types\n",
        "    \"\"\"\n",
        "    print('Report of Initial Data Analysis:\\n')\n",
        "    print(f'Shape of dataframe: {df.shape}')\n",
        "    print(f'Features and Data Types: \\n {df.dtypes}')\n",
        "    print(\"DataFrame Row Number: \", df.shape[0])\n",
        "    print(\"Unique IDs: \", df.ID.nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpdHv51gHzhe",
        "colab_type": "text"
      },
      "source": [
        "#### Percentage of missing value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orF9TRY3j4Xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percent_missing(df):\n",
        "    \"\"\"\n",
        "    Given a dataframe it calculates the percentage of missing records per column\n",
        "    Params:\n",
        "        - df\n",
        "    Returns:\n",
        "        - Dictionary of column name and percentage of missing records\n",
        "    \"\"\"\n",
        "    col=list(df.columns)\n",
        "    perc=[round(df[c].isna().mean()*100,2) for c in col]\n",
        "    miss_dict=dict(zip(col,perc))\n",
        "    return miss_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sR8OYybH3b_",
        "colab_type": "text"
      },
      "source": [
        "#### Missing value exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3TPKQOQwP_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# missing values in a column, especially response variable\n",
        "def missing_class(df, col_name):\n",
        "  \"\"\"\n",
        "      Given a dataframe and colume, it calculates the count of missing records\n",
        "    Params:\n",
        "        - df: dataframe\n",
        "        - col_name : col_name\n",
        "    Returns:\n",
        "        - number of records\n",
        "  \"\"\"\n",
        "  missing_vals = [\"No idea\", np.nan, \"#\", \"?\"]\n",
        "  print(\"Number of missing rows within \" + col_name +\" :\"  + str(df[col_name].isnull().sum(axis=0)))\n",
        "\n",
        "  # replace missing or abnormal value with np.nan\n",
        "  df[col_name] = df[col_name].replace(missing_vals, np.nan)\n",
        "\n",
        "  # dropping na\n",
        "  df.dropna(subset = [col_name], inplace=True)\n",
        "  \n",
        "  # check the cleaned colname\n",
        "  print(df.groupby(col_name).size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq66FiLTC15p",
        "colab_type": "text"
      },
      "source": [
        "#### Converting data types for selected columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojU2US0ry1vM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# covert dataframe columns to numeric or other types\n",
        "\n",
        "def convert_num(df, columns):\n",
        "  \"\"\"\n",
        "      Given a dataframe and colume, it convert to another data type\n",
        "    Params:\n",
        "        - df: dataframe\n",
        "        - columns : list of columns\n",
        "    Returns:\n",
        "        - dataframe with converted data type\n",
        "  \"\"\"\n",
        "  for i in range(0, len(columns)):\n",
        "    df[columns[i]] = df[columns[i]].astype(\"int64\")\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOx9_LWrDAiM",
        "colab_type": "text"
      },
      "source": [
        "#### Duplicate check, basically two situations\n",
        "1. duplicated identical rows\n",
        "2. same ID with different values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9l0reNuDGMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# any duplicate rows? especially in ID\n",
        "\n",
        "def duplicate_row_remove(df, col_name):\n",
        "  \"\"\"\n",
        "      Given a dataframe and colume, remove duplicated rows\n",
        "    Params:\n",
        "        - df: dataframe\n",
        "        - col_name(str): a column name, e.g \"ID\"\n",
        "    Returns:\n",
        "        - number of remaining rows\n",
        "  \"\"\"\n",
        "  global breast # dataframe name\n",
        "  # original length of dataframe\n",
        "  original_length = len(df)\n",
        "  # Number of unique IDs\n",
        "  print(\"Number of unique IDs: \" + str(df[col_name].nunique()))\n",
        "  # remove duplicated rows\n",
        "  df = df[~df.loc[:, col_name:].duplicated()]\n",
        "\n",
        "  # new length of dataframe\n",
        "  new_length = len(df)\n",
        "  # count of rows removed\n",
        "  rows_removed = original_length - new_length\n",
        "  print(\"Number of identical replicated rows should removed: \" + str(rows_removed))\n",
        "  print(\"Number of remaining rows: \" + str(len(df)))\n",
        "  breast = df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReQFVhSNJ11M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# same user ID with different values\n",
        "\n",
        "def duplicate_ID(df, col_name):\n",
        "  \"\"\"\n",
        "      Given a dataframe and colume, remove non-identical rows with same ID\n",
        "    Params:\n",
        "        - df: dataframe\n",
        "        - col_name: a column name, e.g ID\n",
        "    Returns:\n",
        "        - number of remaining rows\n",
        "  \"\"\"\n",
        "  global breast # dataframe name\n",
        "  ID_dup = df[df[col_name].duplicated()]\n",
        "  print(\"Number of duplicated ID: \" + str(len(ID_dup)))\n",
        "\n",
        "  # keep the smallest index for each user\n",
        "  df = df.sort_values(by= [col_name, \"Index\"], ascending = True)\n",
        "  df = df[~df[col_name].duplicated(keep = 'first')]\n",
        "  print(\"Number of remaining rows: \" + str(len(df)))\n",
        "  breast = df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzY4Q7oxKZ4D",
        "colab_type": "text"
      },
      "source": [
        "#### Check and remove out-ranged Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_XumbPXKajF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Incorrect_feature_values(df,columns, val, col_name ):\n",
        "  \"\"\"\n",
        "      Given a dataframe and value, remove col_name's value > val\n",
        "    Params:\n",
        "        - df(dataframe): dataframe\n",
        "        - col_name (str): a column name, e.g \"ID\"\n",
        "        - val(int): specific value \n",
        "        - columns(list): columns list that need to check the value range\n",
        "    Returns: dataframe removed rows with out-ranged value\n",
        "  \"\"\"\n",
        "  global breast\n",
        "  rows_with_large_vals = []\n",
        "  for col in range(0, len(columns)):\n",
        "    filter_by_col = df[columns[col]] > val\n",
        "    ID_vals = pd.array(df[filter_by_col][col_name])\n",
        "    rows_with_large_vals.append(ID_vals)\n",
        "\n",
        "  # group list of lists into 1 list\n",
        "  flat_list = []\n",
        "  for sublist in rows_with_large_vals:\n",
        "    for item in sublist:\n",
        "      flat_list.append(item)\n",
        "  \n",
        "  # list of users with out range values\n",
        "  users_with_large_vals = pd.array(flat_list).unique()\n",
        "  print(\"There are \"+ str(len(users_with_large_vals)) + \" users with values that exceed \" + str(val))\n",
        "\n",
        "  # remove record with any out-range values\n",
        "  df = df[~df[col_name].isin(pd.array(users_with_large_vals))]\n",
        "  breast = df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD1TGPkKEF_Q",
        "colab_type": "text"
      },
      "source": [
        "#### Re-assign value to a column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBngui01j_FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# rename the class based on code\n",
        "def reClass(df, col_name):\n",
        "  \"\"\"\n",
        "      Given a dataframe and col_name, re-assign value according to multiple conditions\n",
        "    Params:\n",
        "        - df(dataframe): dataframe\n",
        "        - col_name (str): a column name, e.g \"ID\"\n",
        "    Returns: dataframe's column with re-assigned value\n",
        "  \"\"\"\n",
        "  global breast\n",
        "  class_new = []\n",
        "  for item in df[col_name]:\n",
        "    if item == 2:\n",
        "      class_new.append(0)\n",
        "    elif item ==  4:\n",
        "      class_new.append(1)\n",
        "    else:\n",
        "      class_new.append(np.nan)\n",
        "  #df.drop(df_col)\n",
        "  df['Class_new'] = class_new\n",
        "  # drop original column\n",
        "  df = df.drop(col_name, axis = 1)\n",
        "  # assign new column as original column name\n",
        "  df = df.rename(columns = {'Class_new': col_name})\n",
        "  # remove NA\n",
        "  df = df[df[col_name] != np.nan]\n",
        "  breast = df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr0DSs1gFoSy",
        "colab_type": "text"
      },
      "source": [
        "#### Aggregation summary of each columns in a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWmhLA1Ij_Lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check the aggregation for each column\n",
        "def col_agg(df, col_list):\n",
        "  col_summary = []\n",
        "  for name in col_list:\n",
        "    col_summary.append(df.groupby(name).size().reindex())\n",
        "  return col_summary\n",
        "\n",
        "# eg.  col_agg(breast, breast.columns[1:12])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAJyFmTYKu2b",
        "colab_type": "text"
      },
      "source": [
        "# <font color = 'blue'> 2. Data Exploratory Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKiGcO0BZicF",
        "colab_type": "text"
      },
      "source": [
        "#### Set Profile Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRsiXgP3ZphO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install --upgrade pandas_profiling\n",
        "def profile(df):\n",
        "\n",
        "  \"\"\"\n",
        "    Given a dataframe, return data profile \n",
        "    Params:\n",
        "      - df(dataframe): dataframe\n",
        "    Returns: data profile in html format\n",
        "   \"\"\"\n",
        "  global prof\n",
        "  from pandas_profiling import ProfileReport\n",
        "  prof = ProfileReport(df)\n",
        "  prof.to_file(output_file= \"data_profile.html\")\n",
        "  return prof"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtmDYtMYxeoz",
        "colab_type": "text"
      },
      "source": [
        "#### Count plot in Seaborn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3zqpqupxeOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sub_countplot(df, Class):\n",
        "  \"\"\"\n",
        "    Given a dataframe, return data profile \n",
        "    Params:\n",
        "      - df(dataframe): dataframe\n",
        "      - Class(str): col_name, e.g \"Class\", usually response variable\n",
        "    Returns: count_plot by features\n",
        "  \"\"\" \n",
        "  features = df.columns[1:-1].to_list()\n",
        "  feature_num = len(features)\n",
        "  x = 3\n",
        "  y = feature_num//3 + feature_num%3\n",
        "  fig, ax = plt.subplots(x, y, figsize= (15,15))\n",
        "  for i in range(feature_num):\n",
        "    sns.countplot(x= features[i], hue= Class, data = df, ax = ax[i//3, i%3])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2M6naRGHm5X",
        "colab_type": "text"
      },
      "source": [
        "#### Check normality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm1pH6czkE8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normality_test(df,col_list):\n",
        "    \"\"\"\n",
        "    Given a dataframe determines whether each numerical column is Gaussian \n",
        "    H0 = Assumes distribution is not Gaussian\n",
        "    Ha = Assumes distribution is Gaussian\n",
        "    Params:\n",
        "        - df\n",
        "    Returns:\n",
        "        - W Statistic\n",
        "        - p-value\n",
        "        - List of columns that do not have gaussian distribution\n",
        "    \"\"\"\n",
        "    non_gauss=[]\n",
        "    w_stat=[]\n",
        "    # Determine if each sample of numerical feature is gaussian\n",
        "    alpha = 0.05\n",
        "    for n in col_list:\n",
        "        stat,p=shapiro(df[n])\n",
        "        print(sns.distplot(df[n]))\n",
        "        print(n, \"skew is:\", skew(df[n]), \"kurtosis is :\", kurtosis(df[n]))\n",
        "\n",
        "\n",
        "        if p <= alpha: # Reject Ho -- Distribution is not normal\n",
        "            non_gauss.append(n)\n",
        "            w_stat.append(stat)\n",
        "    # Dictionary of numerical features not gaussian and W-Statistic        \n",
        "    norm_dict=dict(zip(non_gauss,w_stat))\n",
        "    return norm_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OgVV_Q8BzCR",
        "colab_type": "text"
      },
      "source": [
        "#### Outliers by Boxplot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8e-W3oXkH74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Outliers by boxplot\n",
        "col_names = breast.columns[2:11]\n",
        "col_names\n",
        "\n",
        "breast.boxplot(column=['Clump Thickness', 'Uniformity of Cell Size',\n",
        "       'Uniformity of Cell Shape', 'Marginal Adhesion',\n",
        "       'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\n",
        "       'Normal Nucleoli','Mitoses' ], grid=False, rot=90, fontsize=11)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go3_x8lwB31J",
        "colab_type": "text"
      },
      "source": [
        "#### Data transformation by boxcox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exDYh_0JkKXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data transformation?\n",
        "from scipy.stats import boxcox\n",
        "plt.hist(boxcox(breast['Clump Thickness'],1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSpmgkzTB9Q8",
        "colab_type": "text"
      },
      "source": [
        "#### Visualize the binary classes count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG0o_fCRkNJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def class_fig(col):\n",
        "  \"\"\"\n",
        "  Given the response variable, it visulize the count in binary classes\n",
        "  Params:\n",
        "    - Series\n",
        "  Returns:\n",
        "    - countplot\n",
        "  \"\"\"\n",
        "  sns.countplot(col, label = 'Count')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGgrsBJHkNcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGcHugGUCDJH",
        "colab_type": "text"
      },
      "source": [
        "#### Correlation heatmap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNRi0KYykNfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# correlation between features\n",
        "\n",
        "def corr_heatmap(df, col_names):\n",
        "  corr = df[col_names].corr()\n",
        "  mask = np.zeros_like(corr)\n",
        "  mask[np.triu_indices_from(mask)] = True\n",
        "  sns.heatmap(corr, annot= True, vmin= -1, vmax= 1, mask= mask )\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkEaiHBxCHa5",
        "colab_type": "text"
      },
      "source": [
        "#### Feature Importance by Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUkTGiD-kNmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature importance by Random Forest\n",
        "\n",
        "def rf_importance(X,y):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.3, random_state = 42)\n",
        "  rf = RandomForestClassifier()\n",
        "  rf.fit(X_train,y_train)\n",
        "  \n",
        "  # Feature Importance\n",
        "  importance_rf = pd.Series(rf.feature_importances_, index = X_train.columns)\n",
        "  sorted_importance_rf = importance_rf.sort_values()\n",
        "  sorted_importance_rf.plot(kind = 'barh', color = 'lightgreen')\n",
        "  plt.title(\"Feature Importance by Random Forest\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne3lEQ_HLLNT",
        "colab_type": "text"
      },
      "source": [
        "# <font color = 'blue'> 3. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG35hKLA0fH6",
        "colab_type": "text"
      },
      "source": [
        "#### <font color = 'red'> Model_util package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTowCSDO0fXg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "31ba4110-9800-4276-b4cb-d42f6bffe64b"
      },
      "source": [
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib \n",
        "\n",
        "from sklearn.metrics import confusion_matrix, cohen_kappa_score, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, PredefinedSplit, KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from xgboost.sklearn import XGBRegressor, XGBClassifier\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "\n",
        "class ClassifierModel(object):\n",
        "    \"\"\"\n",
        "    A wrapper class for regression models.\n",
        "    It can be used for training and prediction.\n",
        "    Can plot feature importance and training progress (if relevant for model).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_wrapper=None):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "            columns (list): \n",
        "            model_wrapper: \n",
        "        \"\"\"\n",
        "        self.model_wrapper = model_wrapper\n",
        "        \n",
        "        \n",
        "    def save_model(self, save_path):\n",
        "        joblib.dump(self, save_path, compress = 1)\n",
        "\n",
        "        \n",
        "    def fit(self, X, y,\n",
        "            n_splits,\n",
        "            params=None,\n",
        "            eval_metric='logloss',\n",
        "            plot=True,\n",
        "            plot_title=None,\n",
        "            verbose=1):\n",
        "        \n",
        "        \"\"\"\n",
        "        Training the model.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame), y: training data. \n",
        "            n_splits: cross-validation splits the data. \n",
        "            params (dict): training parameters. Including hyperparameters and:\n",
        "                params['objective'] (str): 'regression' or 'classification',\n",
        "                params['verbose'] (bool),\n",
        "                params['cat_cols'] (list): categorical_columns, only used in LGB and CatBoost wrappers.\n",
        "                params['early_stopping_rounds'] (int).\n",
        "            eval_metric (str): metric for validataion.\n",
        "            plot (bool): if true, plot 'feature importance', 'training curve', 'distribution of prediction', 'distribution of error'.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.eval_metric = eval_metric\n",
        "        self.verbose = verbose\n",
        "        folds = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "        self.columns = X.columns.to_list()\n",
        "        \n",
        "        self.models = []  # if n_splits=5, save 5 models.\n",
        "        self.scores = []  # if n_splits=5, save 5 score items. Each is like: {'validation_0': {'rmse': 396.888855}, 'validation_1': {'rmse': 417.889496}}\n",
        "        self.feature_importances = pd.DataFrame(columns=['feature', 'gain'])  #  if n_splits=5, then self.feature_importances is the stack of the 5 models.\n",
        "        self.oof = np.empty(X.shape[0])   # Predicted results using cross-validation. OOF: \"Out-of-fold\".\n",
        "        self.oof[:] = np.NaN\n",
        "        \n",
        "        for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
        "            X_train, X_valid = X.iloc[train_index].copy(), X.iloc[valid_index].copy()\n",
        "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "            model = copy.deepcopy(self.model_wrapper)\n",
        "            model.fit(X_train, y_train, X_valid, y_valid, params=params)\n",
        "            \n",
        "            self.models.append(model)\n",
        "            self.scores.append(model.best_score_)\n",
        "            self.oof[valid_index] = model.predict(X_valid).reshape(-1,)\n",
        "            \n",
        "            fold_importance = pd.DataFrame({\n",
        "                                    'feature': X_train.columns,\n",
        "                                    'gain': model.feature_importances_\n",
        "                                })\n",
        "            self.feature_importances = self.feature_importances.append(fold_importance)\n",
        "            \n",
        "            if self.verbose > 1:\n",
        "                print(f'\\nFold {fold_n} started.')\n",
        "                for val in model.best_score_.keys():\n",
        "                    print(f\"{self.eval_metric} score on {val}: {model.best_score_[val][self.eval_metric]:.3f}.\")\n",
        "\n",
        "        self.calc_scores_()\n",
        "        \n",
        "        if plot:\n",
        "            # print(classification_report(y, self.oof.argmax(1)))\n",
        "            fig, ax = plt.subplots(figsize=(32, 8))\n",
        "            plt.subplot(1, 3, 1)\n",
        "            self.plot_feature_importance()\n",
        "            plt.subplot(1, 3, 2)\n",
        "#             self.plot_feature_importance()\n",
        "            self.plot_learning_curve()\n",
        "#             plt.subplot(1, 4, 3)\n",
        "#             self.plot_learning_curve()\n",
        "    \n",
        "    \n",
        "    def predict(self, X_test, averaging='usual'):\n",
        "        \"\"\"\n",
        "        Make prediction\n",
        "\n",
        "        Args:\n",
        "            X_test (pd.DataFrame): test data\n",
        "            averaging: method of averaging\n",
        "            \n",
        "        Return:\n",
        "            list: prediction of X_test\n",
        "        \"\"\"\n",
        "        \n",
        "        full_prediction = np.zeros(X_test.shape[0])\n",
        "        for i in range(len(self.models)):\n",
        "            y_pred = self.models[i].predict(X_test).reshape(-1)\n",
        "            if averaging == 'usual':\n",
        "                full_prediction += y_pred\n",
        "            elif averaging == 'rank':\n",
        "                full_prediction += pd.Series(y_pred).rank().values\n",
        "        return full_prediction / len(self.models)\n",
        "        \n",
        "\n",
        "    def calc_scores_(self):\n",
        "        \"\"\"\n",
        "        Average the scores from the n_splits cross validation.\n",
        "        \"\"\"\n",
        "        self.ave_scores = {}\n",
        "        sets = [k for k in self.scores[0]]  # sets = ['validation_0', 'validation_1']\n",
        "        print(f'\\nFinished cross-validation training.')\n",
        "        for val in sets:\n",
        "            scores = [score[val][self.eval_metric] for score in self.scores]\n",
        "            if self.verbose:\n",
        "                print(f\"CV mean {self.eval_metric} score on {val}: {np.mean(scores):.3f} +/- {np.std(scores):.3f} std.\")\n",
        "            self.ave_scores[val] = np.mean(scores)  # self.ave_scores: {'validation_0': 398.9524596, 'validation_1': 408.9034486}\n",
        "\n",
        "\n",
        "    def plot_feature_importance(self, drop_null_importance=True, top_n=20):\n",
        "        \"\"\"\n",
        "        Plot feature importance.\n",
        "\n",
        "        Args:\n",
        "            drop_null_importance (bool): drop columns with null feature importance\n",
        "            top_n (int): show top n features.\n",
        "        \"\"\"\n",
        "        \n",
        "#         fig = plt.figure(figsize=(8, 8))\n",
        "        top_feats = self.get_top_features(drop_null_importance, top_n)\n",
        "        feature_importances = self.feature_importances.loc[self.feature_importances.loc[:, 'feature'].isin(top_feats)]\n",
        "        feature_importances.loc[:, 'feature'] = feature_importances.loc[:, 'feature'].astype(str)\n",
        "        top_feats = [str(i) for i in top_feats]\n",
        "        sns.barplot(data=feature_importances, x='gain', y='feature', orient='h', order=top_feats)\n",
        "        plt.title(\"Feature Importance\")\n",
        "    \n",
        "\n",
        "    \n",
        "    def get_top_features(self, drop_null_importance=True, top_n=20):\n",
        "        \"\"\"\n",
        "        Get top features by importance.\n",
        "        \n",
        "        Args:\n",
        "            drop_null_importance (bool): drop columns with null feature importance\n",
        "            top_n (int): show top n features.\n",
        "        \"\"\"\n",
        "        \n",
        "        grouped_feats = self.feature_importances.groupby(['feature'])['gain'].mean()  # average over folds.\n",
        "        if drop_null_importance:\n",
        "            grouped_feats = grouped_feats[grouped_feats != 0]\n",
        "        return list(grouped_feats.sort_values(ascending=False).index)[:top_n]\n",
        "\n",
        "    \n",
        "    def plot_learning_curve(self):\n",
        "        \"\"\"\n",
        "        Plot training learning curve.\n",
        "        Inspired by `plot_metric` from https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html\n",
        "        \n",
        "        An example of model.evals_result_: \n",
        "            {\n",
        "                'validation_0': {'rmse': [0.259843, 0.26378, 0.26378, ...]},\n",
        "                'validation_1': {'rmse': [0.22179, 0.202335, 0.196498, ...]}\n",
        "            }\n",
        "            \n",
        "            'validation_0' represent train set;\n",
        "            'validation_1' represent validation set;\n",
        "        \"\"\"\n",
        "        \n",
        "#         fig = plt.figure(figsize=(8, 8))\n",
        "        full_evals_results = pd.DataFrame()\n",
        "        for model in self.models:\n",
        "            evals_result = pd.DataFrame()\n",
        "            for k in model.model.evals_result_.keys():  # iterate through different sets.\n",
        "                evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n",
        "            evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n",
        "            full_evals_results = full_evals_results.append(evals_result)\n",
        "\n",
        "        full_evals_results = full_evals_results.melt(id_vars=['iteration']).rename(columns={'value': self.eval_metric,\n",
        "                                                                                            'variable': 'dataset'})\n",
        "        sns.lineplot(data=full_evals_results, x='iteration', y=self.eval_metric, hue='dataset')\n",
        "        plt.title('Train Learning-Curve')\n",
        "        \n",
        "        \n",
        "#################################################################\n",
        "# Model Wrappers.\n",
        "#################################################################\n",
        "\n",
        "class RandForest_regr(object):\n",
        "    \"\"\"\n",
        "    A wrapper for sklearn RandomForestRegressor model so that we will have a single api for various models.\n",
        "    \n",
        "    Example of params:\n",
        "    params = { \n",
        "        'n_estimators': 100,\n",
        "        'criterion': 'mse',\n",
        "        'max_depth': 7,\n",
        "        'min_samples_split': 2,\n",
        "        'n_jobs': -1,\n",
        "        'random_state': 123,\n",
        "        'verbose': 0,\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = RandomForestRegressor()\n",
        "        \n",
        "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, params=None):\n",
        "        self.model.set_params(**params)\n",
        "        self.model.fit(X=X_train, y=np.array(y_train).reshape(-1))\n",
        "        score = mean_squared_error(y_train, self.model.predict(X_train))\n",
        "        self.best_score_ = score\n",
        "        self.feature_importances_ = self.model.feature_importances_\n",
        "        \n",
        "    def predict(self, X_test):\n",
        "        return self.model.predict(X_test)\n",
        "        \n",
        "        \n",
        "\n",
        "class XGBWrapper_clf(object):\n",
        "    \"\"\"\n",
        "    A wrapper for xgboost model so that we will have a single api for various models.\n",
        "    \n",
        "    Example of params:\n",
        "    params = { \n",
        "        'n_estimators': 50,  #################\n",
        "        'max_depth':  3,  #################\n",
        "        'learning_rate': 0.01, \n",
        "    #     'min_child_weight': np.arange(1, 4, 1),\n",
        "    #     'gamma': np.arange(0, 0.03, 0.01),\n",
        "    #     'reg_alpha': np.arange(0, 0.01, 0.003),\n",
        "        'objective': 'reg:squarederror', #['reg:squaredlogerror']#, # squared loss.\n",
        "        'verbose': 0,\n",
        "        'early_stopping_rounds': None,\n",
        "        'n_jobs': -1,\n",
        "        'random_state': 123\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = XGBClassifier()\n",
        "\n",
        "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, params=None):\n",
        "\n",
        "        self.model = self.model.set_params(**params)\n",
        "        \n",
        "        eval_set = [(X_train, y_train)]\n",
        "        if X_valid is not None:\n",
        "            eval_set.append((X_valid, y_valid))\n",
        "\n",
        "        self.model.fit(X=X_train, y=y_train,\n",
        "                       eval_set=eval_set, eval_metric='logloss',\n",
        "                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'])\n",
        "\n",
        "        scores = self.model.evals_result()\n",
        "        self.best_score_ = {k: {m: m_v[-1] for m, m_v in v.items()} for k, v in scores.items()}\n",
        "#         self.best_score_ = {k: {m: n if m != 'cappa' else -n for m, n in v.items()} for k, v in self.best_score_.items()}\n",
        "\n",
        "        self.feature_importances_ = self.model.feature_importances_\n",
        "    \n",
        "    def predict(self, X_test):\n",
        "        return self.model.predict(X_test)\n",
        "\n",
        "#     def predict_proba(self, X_test):\n",
        "#         if self.model.objective == 'binary':\n",
        "#             return self.model.predict_proba(X_test, ntree_limit=self.model.best_iteration)[:, 1]\n",
        "#         else:\n",
        "#             return self.model.predict_proba(X_test, ntree_limit=self.model.best_iteration)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGHWjHx7CMRT",
        "colab_type": "text"
      },
      "source": [
        "#### SMOTE for resampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXLEk6UdkVAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SMOTE for Upsampling training \n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "X_train, X_test, y_train, y_test = train_test_split( breast[breast.columns[0:8]], breast['Class'], test_size= 0.3, random_state = 42)\n",
        "X_res, y_res = SMOTE(random_state = 42).fit_sample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsfUdfnXLadG",
        "colab_type": "text"
      },
      "source": [
        "#### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEl4zGuBLYF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = pd.DataFrame(preprocessing.normalize(X_train))\n",
        "X_test = pd.DataFrame(preprocessing.normalize(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7AMXt8mMHlm",
        "colab_type": "text"
      },
      "source": [
        "#### Standardlization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUmnQdG3MEwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# standardlization\n",
        "\n",
        "ss = StandardScaler()\n",
        "train_X = ss.fit_transform(train_X)\n",
        "test_X = ss.fit_transform(test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJdn-WXSCQ9Y",
        "colab_type": "text"
      },
      "source": [
        "#### Logistic Regression with sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyO5g6L3kW1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistic Regression \n",
        "\n",
        "def LogReg(X_res, y_res, X_test, y_test):\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "  from sklearn.metrics import classification_report, confusion_matrix\n",
        "  from sklearn.metrics import roc_auc_score\n",
        "  from sklearn.metrics import roc_curve\n",
        "  logreg = LogisticRegression().fit(X_res, y_res)\n",
        "  y_pred = logreg.predict(X_test)\n",
        "  print(\"confusion_matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "  print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
        "  print(\"ROC:\", roc_auc_score(y_test, y_pred))\n",
        "\n",
        "  # compute predicted probabilites: y_pred_prob\n",
        "  y_pred_prob = logreg.predict_log_proba(X_test)[:, 1]\n",
        "\n",
        "  # generate ROC curve\n",
        "\n",
        "  fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "\n",
        "  # plot ROC curve\n",
        "\n",
        "  plt.plot([0,1], [0,1], 'k--')\n",
        "  plt.plot(fpr, tpr)\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('ROC Curve')\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_5js3yiMzVn",
        "colab_type": "text"
      },
      "source": [
        "#### Logistic Regression with statsmodels.api"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF8ILE7bMl_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logit_SM(X_train, y_train):\n",
        "  import statsmodels.api as sm\n",
        "  logit_model = sm.Logit(y_train.values.ravel(), X_train)\n",
        "  result = logit_model.fit()\n",
        "  print(result.summary2())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtcp9RwSCTon",
        "colab_type": "text"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMOViXgpkY8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# random Forest\n",
        "def RF_pipe(X_res, y_res, X_test, y_test):\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "  from sklearn.metrics import classification_report, confusion_matrix\n",
        "  from sklearn.metrics import roc_auc_score\n",
        "  from sklearn.metrics import roc_curve\n",
        "  rf = RandomForestClassifier().fit(X_res, y_res)\n",
        "  y_pred = rf.predict(X_test)\n",
        "  print(\"confusion_matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "  print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
        "  print(\"ROC:\", roc_auc_score(y_test, y_pred))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjPzmCijOrbU",
        "colab_type": "text"
      },
      "source": [
        "# <font color = 'green'> Example 1: Conversion rate\n",
        "The goal of this challenge is to build a model that predicts conversion rate and, based on the model, come up with ideas to improve it.\n",
        "\n",
        "We have data about all users who hit our site: whether they converted or not as well as some of their characteristics such as their country, the marketing channel, their age, whether they are repeat users and the number of pages visited during that session (as a proxy for site activity/time spent on site).\n",
        "\n",
        "Your project is to:\n",
        "\n",
        "1.   Predict conversion rate\n",
        "2.   Come up with recommendations for the product team and the marketing team to improve conversion rate\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZmLzoIWO1Kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbtkTo5rTQ1a",
        "colab_type": "text"
      },
      "source": [
        "## Data Exploration with Seaborn\n",
        "[Seaborn Gallary](https://seaborn.pydata.org/examples/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfHhM6H_T72s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "327f9a36-80c3-4637-bf19-9e3cb3d17f3d"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/M- DataMarked/01- TakeHomeDataChallenges-master (20 é¢˜)/01.ConversionRate/conversion_data.csv\")\n",
        "data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "      <th>age</th>\n",
              "      <th>new_user</th>\n",
              "      <th>source</th>\n",
              "      <th>total_pages_visited</th>\n",
              "      <th>converted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>UK</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>Ads</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>Seo</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US</td>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>Seo</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>China</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>Seo</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>Seo</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  country  age  new_user source  total_pages_visited  converted\n",
              "0      UK   25         1    Ads                    1          0\n",
              "1      US   23         1    Seo                    5          0\n",
              "2      US   28         1    Seo                    4          0\n",
              "3   China   39         1    Seo                    5          0\n",
              "4      US   30         1    Seo                    6          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}